{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2022."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Data pipeline in TensorFlow: Understanding the data pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This is a tutorial for beginners to learn the TensorFlow data pipeline (`tf.data`), its usage with high level tf.keras APIs, and `tf.data` related operations that could be used for pipeline-level data processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup and model preparation\n",
        "\n",
        "Before showing the usage of `tf.data`, you can go through the following steps to setup the environment in Google Colab and build a very simple model with `tf.keras`, so that it could be used with `tf.data` later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrS6CbVZ5pYB"
      },
      "source": [
        "Now import TensorFlow into your program:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw__PMrO5qSf"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3iMfanb7jlm"
      },
      "source": [
        "Build a tf.keras.Sequential model by stacking layers. For demo purposes choose a very simple model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc4qLkq37lJq"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfesJ_VB8GlJ"
      },
      "source": [
        "Choose an optimizer and loss function for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rza-od2i8q8J"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Load the Fashion-MNIST dataset\n",
        "\n",
        "The dataset used in this tutorial is [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), which consists of Zalando's article images with a training set of 60,000 examples and a test set of 10,000 examples. For simplicity reasons we use tf.keras to load the data into numpy. The `images` are converted to `float32` and `labels` are converted to `int32`:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUj0878jPyz7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "images, labels = train\n",
        "images = images.astype(np.float32)/255.0\n",
        "labels = labels.astype(np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6qwJkYX82LS"
      },
      "source": [
        "The next step is to convert the loaded numpy data into a `tf.data.Dataset`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CNom1-nQGEW"
      },
      "source": [
        "d_train = tf.data.Dataset.from_tensor_slices((images, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFIMPrUrQdjy"
      },
      "source": [
        "A `tf.data.Dataset` is ready, but what exactly is `tf.data.Dataset`?\n",
        "\n",
        "You could check the property of `element_spec` to find out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeqCxVcDQ-1Y"
      },
      "source": [
        "print(\"d_train: {}\".format(d_train.element_spec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5kfASylR9qR"
      },
      "source": [
        "The output of the `element_spec` lists the details of the dataset:\n",
        "```\n",
        "(\n",
        "  TensorSpec(shape=(28, 28), dtype=tf.float32, name=None),\n",
        "  TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
        ")\n",
        "```\n",
        "\n",
        "Turns out the dataset is a series of tuples where the first element of the tuple is a `28x28` image while the second element of the tuple is a label of `int32` scalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyrZVnRIS3ch"
      },
      "source": [
        "## Supported operations of `tf.data.Dataset`\n",
        "\n",
        "Many usefule operations are supported by `tf.data.Dataset`.  For example, `take(n)` will take the first `n` elements at the beginning of the dataset. The dataset can also be iterated with `for` loop through implicit `__iter__` call:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKRo2TJgUzHb"
      },
      "source": [
        "for image, label in d_train.take(2):\n",
        "  print(\"image: {}\\nlabel: {}\\n\".format(image, label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRHNZHolVMIY"
      },
      "source": [
        "The `map(func)` is a very useful operation that applies `func` to each elements of the dataset, and returns a new dataset after the transformations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG_5GiHEVpSn"
      },
      "source": [
        "d_train_size = d_train.map(lambda image, label: (tf.size(image), tf.size(label)))\n",
        "\n",
        "# expected image_size is 784 = 28x28 and label_size is 1:\n",
        "for image_size, label_size in d_train_size.take(2):\n",
        "  print(\"image_size: {}\\nlabel_size: {}\\n\".format(image_size, label_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KEukRJMWPh2"
      },
      "source": [
        "A complete list of supported operations for `tf.data.Dataset` is available in the [api documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1TE04RmYDwB"
      },
      "source": [
        "## Usage of `tf.data` and `tf.keras`\n",
        "\n",
        "Before `tf.data.Dataset` is used by `tf.keras`, the dataset is normally shuffled and batched:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr0dQWcvY96C"
      },
      "source": [
        "d_train = d_train.shuffle(5000).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kVQdH1g9IH6"
      },
      "source": [
        "Recall a `model` has already be compiled at the beginning of the tutorial, it is now possible to directly use `tf.data` with `mode..fit` within `tf.keras`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w41ySNNj9Uud"
      },
      "source": [
        "model.fit(d_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzfqdzel-J8N"
      },
      "source": [
        "The `Model.evaluate` method accepts `tf.data.Dataset` as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRbt58jo-UHW"
      },
      "source": [
        "model.evaluate(d_train, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyf5TC44_It6"
      },
      "source": [
        "Finally, `model.predict` could also takes a `tf.data.Dataset` as an input for inference. But instead of a tuple of `(image, label)` pairs. The dataset passed to `model.predict` only need `image` (no `label`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY9HIJrq_O_5"
      },
      "source": [
        "# image only dataset\n",
        "d_image = tf.data.Dataset.from_tensor_slices(images).batch(32)\n",
        "\n",
        "# for prediction\n",
        "model.predict(d_image, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}